Metadata-Version: 2.3
Name: moriarty
Version: 0.0.2
Summary: moriarty
Project-URL: Source, https://github.com/wh1isper/moriarty
Author-email: wh1isper <jizhongsheng957@gmail.com>
License: BSD 3-Clause License
License-File: LICENSE
Keywords: moriarty
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.10
Requires-Dist: pydantic>=2
Provides-Extra: docs
Requires-Dist: autodoc-pydantic; extra == 'docs'
Requires-Dist: pydata-sphinx-theme; extra == 'docs'
Requires-Dist: sphinx; extra == 'docs'
Requires-Dist: sphinx-click; extra == 'docs'
Provides-Extra: test
Requires-Dist: pytest; extra == 'test'
Description-Content-Type: text/markdown

![](https://img.shields.io/github/license/wh1isper/moriarty)
![](https://img.shields.io/github/v/release/wh1isper/moriarty)
![](https://img.shields.io/docker/image-size/wh1isper/moriarty)
![](https://img.shields.io/pypi/dm/moriarty)
![](https://img.shields.io/github/last-commit/wh1isper/moriarty)
![](https://img.shields.io/pypi/pyversions/moriarty)

# moriarty

Moriarty is a set of components for building asynchronous inference cluster.

Relying on cloud vendors or self-built global queue services, asynchronous inference clusters can be built without exposing ports to the public.

## Why asynchronous inference, why moriarty?

- Preventing client timeout.
- Avoid HTTP disconnection due to network or other issues.
- Reducing HTTP queries with queues.
- Deploy on Multi/Hybrid/Private cloud, even on bare metal.

## Alternatives

This project came from my deep use of [Asynchronous Inferenc for AWS Sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html), and as far as I know, only AWS and Aliyun provide asynchronous inference support.

For open source projects, there are many deployment solutions, but most of them are synchronous inference (based on HTTP or RPC).I don't find any alternative for async inference. Maybe Kubeflow pipeline can be used for asynchronous inference. But without serving support(Leave model in GPU as a service, not load per job), there is a significant overhead of GPU memory cache and model load time.

## Architecture Overview

![Architecture Overview](./assets/Architecture.png)

Key Components:

- Matrix: single producer, multiple consumers. `Connector` as producer, provide HTTP API for _Backend Service_ and push invoke request to the global **Job Queue**. `Operator` as consumer, pull tasks from the **Job Queue** and push them to local queue. Pulling or not depends on the load of inference cluster. And also, `Operator` will autoscale inference container if needed.
- Endpoint: Deploy a function as an HTTP service.
- Sidecar: Proxy and transform queue message into HTTP request.
- Init: Init script for inference container

CLIs:

- `moriarty`: Main CLI to interact with `Metrix` service
- `moriarty-deploy`: Request `Metrix Operator`'s API or database for deploy inference endpoint.
- `moriarty-serve`: Start endpoint service, aka endpoint

## Install

`pip install moriarty`

Or use docker image

`docker pull wh1isper/moriarty`

> `docker pull wh1isper/moriarty:dev` for developing version

## Develop

Install pre-commit before commit

```
pip install pre-commit
pre-commit install
```

Install package locally with test dependencies

```
pip install -e .[test]
```

Run tests with pytest

```
pytest -v
```
