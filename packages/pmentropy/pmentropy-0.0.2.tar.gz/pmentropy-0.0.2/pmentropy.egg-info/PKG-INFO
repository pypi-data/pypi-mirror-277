Metadata-Version: 2.1
Name: pmentropy
Version: 0.0.2
Summary: Compute in python entropy for process mining describe in Back, C.O., Debois, S. & Slaats, T. Entropy as a Measure of Log Variability. J Data Semant 8, 129–156 (2019). https://doi.org/10.1007/s13740-019-00105-3 (https://rdcu.be/dJMwH)
Home-page: https://github.com/TavTanguy/pmentropy
Author: Taverna Tanguy
Author-email: taverna.tanguy@gmail.com
License: GNU General Public License v3
Platform: UNKNOWN
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: GNU Affero General Public License v3
Classifier: Programming Language :: Python :: 3.11
Description-Content-Type: text/markdown
License-File: LICENSE

# PMEventropy

Compute in python entropy for process mining describe in Back, C.O., Debois, S. & Slaats, T. Entropy as a Measure of Log Variability. J Data Semant 8, 129–156 (2019). https://doi.org/10.1007/s13740-019-00105-3 ([the article](https://rdcu.be/dJMwH))

This project is inspired by https://github.com/backco/eventropy

## Installation

    pip install pmentropy

## Get started

First import the XES file

    import pmentropy
    logs = pmentropy.read_file("path", flatten=False)

Then compute an entropy

    entropy1 = pmentropy.kNN_entropy(logs, k=3, p=2)
    entropy2 = pmentropy.global_block_entropy(logs k=3, p=2)

## Documentation

Parse file

- read_file(file_path: str, flatten=False)

Entropy

- trace_entropy(logs)
- prefix_entropy(logs)
- unique_trace(logs)
- k_block_entropy(logs, k: int)
- global_block_entropy(logs)
- kL_entropy(logs, p: int)
- kNN_entropy(logs, k: int, p: int)
- lempel_ziv_entropy_rate(logs)
- k_block_entropy_rate_ratio(logs, c)
- k_block_entropy_rate_diff(logs, c)
- unique_trace(logs)


