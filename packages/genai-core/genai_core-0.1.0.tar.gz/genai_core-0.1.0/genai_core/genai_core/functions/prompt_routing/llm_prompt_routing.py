from genai_core.functions.prompt_routing.base_prompt_routing import BasePromptRouting
from genai_core.cores.LLMs.LLM_base import LLM

class LLMPromptRouting(BasePromptRouting):
    """
    A class implementing prompt routing based on responses generated by a large language model (LLM).

    Attributes:
        llm (LLM): An instance of a large language model used for generating responses.
        prompt_routes (Dict[str, str]): A dictionary mapping response types to specific prompt routes.
        routes (List[str]): A list of available prompt routes based on response types.
        prompt (str): The default prompt template if no specific prompt route is matched.
    """
    
    def __init__(self, llm_block: LLM, **kwargs):
        """
        Initializes the LLMPromptRouting with a large language model and optional configurations.

        Args:
            llm_block (LLM): An instance of a large language model used for generating responses.
            **kwargs: Additional keyword arguments including:
                      - prompt_routes (Dict[str, str], optional): A dictionary mapping response types to specific
                        prompt routes.
                      - prompt (str, optional): The default prompt template if no specific prompt route is matched.
        """
        self.llm = llm_block
        self.prompt_routes = kwargs.get("prompt_routes") if "prompt_routes" in kwargs else {}
        self.routes = list(self.prompt_routes.keys())
        self.prompt =  kwargs.get("prompt") if "prompt" in kwargs else "{question}"
        
    def get_prompt(self, question: str, **kwargs) -> [str, str]:
        """
        Generates a prompt based on the response type inferred from the large language model.

        Args:
            question (str): The question for which a prompt is to be generated.
            **kwargs: Additional keyword arguments (not used in this method).

        Returns:
            str: The generated prompt based on the inferred response type.
        """

        llm_answer = self.llm.generate_answer_batch(question=question, prompt_template = self.prompt)
        llm_answer = llm_answer.replace(" ", "")
        flag_found=False
        if (llm_answer[0].lower()=="d") or ("descriptive" in llm_answer.lower()):
            llm_answer="Descriptive"
            flag_found=True
        if ((llm_answer[0].lower()=="o") and (llm_answer[1].lower()=="p")) or ("open" in llm_answer.lower()):
            llm_answer="Open"
            flag_found=True
        if (llm_answer[0].lower()=="e") or ("extractive" in llm_answer.lower()):
            llm_answer="Extractive"
            flag_found=True
        if (llm_answer[0].lower()=="c") or ("confirmative" in llm_answer.lower()):
            llm_answer="Confirmative"
            flag_found=True
        if flag_found==False:
            llm_answer="Base"
            
        return self.prompt_routes.get(llm_answer), llm_answer

            
