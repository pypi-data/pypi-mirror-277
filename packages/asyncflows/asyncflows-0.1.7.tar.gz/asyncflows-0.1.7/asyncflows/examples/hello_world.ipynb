{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# asyncflows Hello World Example\n",
    "\n",
    "This notebook demonstrates the Hello World example with Llama 3, by running Ollama on Google Colab.\n",
    "\n",
    "To use a Google Colab GPU, follow these steps before running the notebook:\n",
    "\n",
    "1. Go to `Edit â€“> Notebook Settings`\n",
    "2. Select a hardware accelerator (T4 GPU is free)"
   ],
   "metadata": {
    "id": "Ne2fpsgePt-5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Install asyncflows\n",
    "\n",
    "!pip install asyncflows\n",
    "\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OMgDxw4MdOOf",
    "outputId": "7acfdd89-336d-4175-d0f3-d3fee2d9afbb",
    "collapsed": true,
    "cellView": "form"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Start Ollama, download Llama 3\n",
    "!curl https://ollama.ai/install.sh | sh\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import threading\n",
    "\n",
    "# NB: You may need to set these depending and get cuda working depending which backend you are running.\n",
    "# Set environment variable for NVIDIA library\n",
    "# Set environment variables for CUDA\n",
    "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
    "# Set LD_LIBRARY_PATH to include both /usr/lib64-nvidia and CUDA lib directories\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'\n",
    "\n",
    "async def run_process(cmd):\n",
    "    print('>>> starting', *cmd)\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        *cmd,\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE\n",
    "    )\n",
    "\n",
    "    # define an async pipe function\n",
    "    async def pipe(lines):\n",
    "        async for line in lines:\n",
    "            print(line.decode().strip())\n",
    "\n",
    "        await asyncio.gather(\n",
    "            pipe(process.stdout),\n",
    "            pipe(process.stderr),\n",
    "        )\n",
    "\n",
    "    # call it\n",
    "    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))\n",
    "\n",
    "async def start_ollama_serve():\n",
    "    await run_process(['ollama', 'serve'])\n",
    "\n",
    "def run_async_in_thread(loop, coro):\n",
    "    asyncio.set_event_loop(loop)\n",
    "    loop.run_until_complete(coro)\n",
    "    loop.close()\n",
    "\n",
    "# Create a new event loop that will run in a new thread\n",
    "new_loop = asyncio.new_event_loop()\n",
    "\n",
    "# Start ollama serve in a separate thread so the cell won't block execution\n",
    "thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n",
    "thread.start()\n",
    "\n",
    "await asyncio.sleep(1)\n",
    "\n",
    "!ollama pull llama3\n",
    "\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yw0evkHrd1uZ",
    "outputId": "987fa52d-fb73-4075-d9cf-ed9c08b54fef",
    "collapsed": true,
    "cellView": "form"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Define the Flow\n",
    "\n",
    "flow_config = \"\"\"\n",
    "\n",
    "default_model:\n",
    "  model: ollama/llama3\n",
    "  max_output_tokens: 50\n",
    "flow:\n",
    "  hello_world:\n",
    "    action: prompt\n",
    "    prompt:\n",
    "      - text: Can you say hello world for me?\n",
    "default_output: hello_world.result\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "nfEAwSIweOem"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Run the Flow\n",
    "\n",
    "from asyncflows import AsyncFlows\n",
    "\n",
    "# Load the flow from the file\n",
    "flow = AsyncFlows.from_text(flow_config)\n",
    "\n",
    "# Run the flow\n",
    "await flow.run()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "F7fkQp58bIe0",
    "outputId": "3b0bbf1a-502b-4df6-ff98-dc570db780d2"
   },
   "execution_count": 7,
   "outputs": []
  }
 ]
}
