Metadata-Version: 2.1
Name: vllm-flash-attn
Version: 2.5.9.post1
Summary: Forward-only flash-attn
Home-page: https://github.com/vllm-project/flash-attention.git
Author: vLLM Team
License: UNKNOWN
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: BSD License
Classifier: Operating System :: Unix
Requires-Python: >=3.8
Requires-Dist: torch ==2.3.1

Forward-only flash-attn package built for PyTorch 2.3.1 and CUDA 12.1


